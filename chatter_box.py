# -*- coding: utf-8 -*-
"""chatter box.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uMBRUbUi1_QMecISXr8j0miSENBJnIbp
"""

!pip install -U langgraph langsmith
!pip install -U langchain-anthropic

!pip install -U langchain-google-genai

# Define the State (can be done once)
from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph.message import add_messages

class State(TypedDict):
    messages: Annotated[list, add_messages]

# Redefine the graph_builder, node, and compile the graph
# This ensures the latest llm is used and avoids adding duplicate nodes
from langgraph.graph import StateGraph, START
from langchain_core.language_models import BaseChatModel

# Ensure llm is defined before this block.
# Assuming llm is already defined as init_chat_model("google_genai:gemini-2.0-flash")

def chatbot(state: State):
    # Ensure 'llm' in this scope refers to the desired language model
    global llm
    return {"messages": [llm.invoke(state["messages"])]}

# Create a new graph builder instance
graph_builder = StateGraph(State)

# Add the chatbot node
graph_builder.add_node("chatbot", chatbot)

# Add the edge from START to chatbot
graph_builder.add_edge(START, "chatbot")

# Compile the graph
graph = graph_builder.compile()

# File ipython-input-2-19440d213a1f (No changes needed here, just defines State)
from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START
from langgraph.graph.message import add_messages

class State(TypedDict):
    messages: Annotated[list, add_messages]

# No graph_builder initialization needed here if you re-initialize later
# graph_builder = StateGraph(State)

# File ipython-input-3-19440d213a1f (Define the function, but don't add the node or compile yet)
from typing import Annotated
from langchain.chat_models import init_chat_model # Keep import
from typing_extensions import TypedDict # Keep import
from langgraph.graph import StateGraph, START # Keep import
from langgraph.graph.message import add_messages # Keep import


# State definition is already in previous cell, no need to repeat
# class State(TypedDict):
#     messages: Annotated[list, add_messages]

# graph_builder initialization will happen later, no need to repeat
# graph_builder = StateGraph(State)

# llm definition will happen later with Google GenAI, no need to repeat here
# llm = init_chat_model("anthropic:claude-3-5-sonnet-latest")

# Define the chatbot function - keep this
def chatbot(state: State):
    # This will use the 'llm' defined later in the notebook
    return {"messages": [llm.invoke(state["messages"])]}

# Do NOT add the node or compile the graph here
# graph_builder.add_node("chatbot", chatbot)
# graph_builder.add_edge(START, "chatbot")
# graph = graph_builder.compile()

# File ipython-input-4-19440d213a1f (Define the preferred llm)
import os
from langchain.chat_models import init_chat_model

os.environ["GOOGLE_API_KEY"] = "AIzaSyB_V3DqJiHPzsbklDmkQQNnSORGPTNnNyo"

llm = init_chat_model("google_genai:gemini-2.0-flash")

# File ipython-input-6-19440d213a1f (Redefine graph_builder, add node, add edge, and compile)
from typing import Annotated # Keep import
from typing_extensions import TypedDict # Keep import
from langgraph.graph import StateGraph, START # Keep import
from langgraph.graph.message import add_messages # Keep import

# State definition is already in previous cell, no need to repeat
# class State(TypedDict):
#     messages: Annotated[list, add_messages]

# chatbot function is already defined, no need to repeat
# def chatbot(state: State):
#     return {"messages": [llm.invoke(state["messages"])]}


# Now, initialize the graph_builder, add the node, add the edge, and compile
graph_builder = StateGraph(State)

# Add the chatbot node - this is the first time adding it to THIS graph_builder instance
graph_builder.add_node("chatbot", chatbot)

# Add the edge
graph_builder.add_edge(START, "chatbot")

# Compile the graph - this is the first time compiling THIS graph_builder instance
graph = graph_builder.compile()

graph_builder.add_edge(START, "chatbot")

from IPython.display import Image, display

try:
    display(Image(graph.get_graph().draw_mermaid_png()))
except Exception:
    # This requires some extra dependencies and is optional
    pass

def stream_graph_updates(user_input: str):
    for event in graph.stream({"messages": [{"role": "user", "content": user_input}]}):
        for value in event.values():
            print("Assistant:", value["messages"][-1].content)


while True:
    try:
        user_input = input("User: ")
        if user_input.lower() in ["quit", "exit", "q"]:
            print("Goodbye!")
            break
        stream_graph_updates(user_input)
    except:
        # fallback if input() is not available
        user_input = "What do you know about LangGraph?"
        print("User: " + user_input)
        stream_graph_updates(user_input)
        break

from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START
from langgraph.graph.message import add_messages
from langchain.chat_models import init_chat_model

class State(TypedDict):
    messages: Annotated[list, add_messages]

graph_builder = StateGraph(State)

# You can use any model you have access to here (Gemini, Claude, GPT, etc.)
llm = init_chat_model("google_genai:gemini-2.0-flash")  # <- use your model

def chatbot(state: State):
    return {"messages": [llm.invoke(state["messages"])]}

graph_builder.add_node("chatbot", chatbot)
graph_builder.add_edge(START, "chatbot")
graph = graph_builder.compile()

def get_bot_reply(user_input):
    for event in graph.stream({"messages": [{"role": "user", "content": user_input}]}):
        for value in event.values():
            return value["messages"][-1].content

import ipywidgets as widgets
from IPython.display import display, clear_output

# TextArea for multiline input
input_box = widgets.Textarea(
    placeholder='Type your message...',
    layout=widgets.Layout(width='80%', height='60px')
)
send_button = widgets.Button(description="Send", button_style='primary')
output_area = widgets.Output(layout={'border': '1px solid black', 'height': '300px', 'overflow_y': 'auto'})

# Display chat UI
chat_ui = widgets.VBox([output_area, widgets.HBox([input_box, send_button])])
display(chat_ui)

# Chat handling function
def on_send_click(_):
    user_msg = input_box.value.strip()
    if not user_msg:
        return

    input_box.value = ""  # Clear input after sending
    with output_area:
        print(f"\nðŸ‘¤ You: {user_msg}")

    # Call LangGraph bot
    response = get_bot_reply(user_msg)

    with output_area:
        print(f"ðŸ¤– Bot: {response}")

# Bind the button click
send_button.on_click(on_send_click)

